import os
import sys
import importlib.util
import tempfile
import torch
import numpy as np
from src.eval import eval_kernel_against_ref, KernelExecResult # From KernelBench
from src.dataset import KERNELBENCH_LEVEL_1_DATASET # From KernelBench
from src.score import fastp # Import KernelBench's scoring function

# For demonstration, we'll focus on a single problem or a small subset.
# In a full implementation, you might want to evaluate on a larger set or all of Level 1.
# PROBLEM_INDICES = range(len(KERNELBENCH_LEVEL_1_DATASET)) # Evaluate all Level 1 problems
PROBLEM_INDICES = [0] # Evaluate only the first problem for now

def evaluate(program_path: str) -> dict:
    """
    Evaluates a program generated by OpenEvolve using KernelBench's methodology.

    Args:
        program_path (str): Path to the Python file generated by OpenEvolve.

    Returns:
        dict: A dictionary containing evaluation metrics, primarily 'combined_score'.
              Adheres to KernelBench's focus on correctness and performance.
    """
    try:
        # 1. Read the generated code
        with open(program_path, 'r') as f:
            generated_code = f.read()

        # 2. Prepare to collect results for all selected problems
        eval_results_per_problem = []
        baseline_times = []
        
        # Assume a fixed baseline time for demonstration. 
        # In practice, these should be loaded from KernelBench's baseline files.
        # For a more accurate setup, you would load these from a JSON file like:
        # `results/timing/{hardware}/{baseline_time_torch}.json`
        # DUMMY_BASELINE_TIME_MS = 10.0
        
        # 3. Evaluate the generated kernel against each selected Level 1 problem
        for problem_index in PROBLEM_INDICES:
            reference_problem_path = KERNELBENCH_LEVEL_1_DATASET[problem_index]

            # 3a. Read the reference problem source code
            with open(reference_problem_path, 'r') as f:
                ref_arch_src = f.read()

            # 3b. Use KernelBench's evaluation function
            # Create a temporary file for the generated kernel code
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as tmp_file:
                tmp_file.write(generated_code)
                tmp_kernel_path = tmp_file.name

            # Read the generated code back as a string to pass to KernelBench
            with open(tmp_kernel_path, 'r') as f:
                custom_model_src = f.read()
            
            # Clean up the temporary file
            os.unlink(tmp_kernel_path)

            # 3c. Run KernelBench evaluation for this specific problem
            device = torch.device("cuda:0") # Assuming evaluation on GPU 0
            eval_result: KernelExecResult = eval_kernel_against_ref(
                original_model_src=ref_arch_src,
                custom_model_src=custom_model_src,
                seed_num=42,
                num_correct_trials=5, # Default from KernelBench for correctness
                num_perf_trials=10,   # Can be adjusted for speed during evolution
                verbose=False,
                measure_performance=True, # Essential for performance metric
                device=device
            )
            
            # Store the result for this problem
            eval_results_per_problem.append(eval_result)
            
            # For demonstration, use a dummy baseline. In practice, load from file.
            baseline_times.append(10.0) 

        # 4. Aggregate results across all problems to compute the final score
        # Following KernelBench's approach:
        # - `is_correct`: Boolean array indicating correctness for each problem.
        # - `baseline_speed`: Array of baseline execution times (from PyTorch) for each problem.
        # - `actual_speed`: Array of generated kernel execution times for each problem.
        # - `n`: Total number of problems evaluated.
        
        n = len(eval_results_per_problem)
        if n == 0:
             return {'combined_score': 0.0, 'error': 'No problems were evaluated.'}

        is_correct = np.array([res.correctness for res in eval_results_per_problem])
        # KernelBench uses `runtime` from KernelExecResult for `actual_speed`
        actual_speed = np.array([res.runtime if res.runtime and res.runtime > 0 else np.inf for res in eval_results_per_problem])
        # Baseline times need to be loaded from KernelBench's data
        baseline_speed = np.array(baseline_times) 
        
        # 5. Calculate combined_score using KernelBench's primary metric: fast_p
        # We will use fastp with p=1.0 as the primary score, which means 
        # "fraction of tasks that are both correct and faster than PyTorch baseline".
        # This directly aligns with KernelBench's core benchmark metric.
        
        # Handle case where baseline speed is not available or zero
        valid_baseline = (baseline_speed > 0) & np.isfinite(baseline_speed)
        if not np.any(valid_baseline):
            # If no valid baselines, fall back to correctness rate (fast_0)
            fast_0_score = np.sum(is_correct) / n
            return {
                'combined_score': float(fast_0_score),
                'details': f'Correctness rate (fast_0) due to missing baselines: {fast_0_score:.4f}',
                'correctness_rate': float(fast_0_score)
            }
            
        # Filter arrays to only include problems with valid baselines
        filtered_is_correct = is_correct[valid_baseline]
        filtered_baseline_speed = baseline_speed[valid_baseline]
        filtered_actual_speed = actual_speed[valid_baseline]
        filtered_n = len(filtered_is_correct)
        
        if filtered_n == 0:
             return {'combined_score': 0.0, 'error': 'No problems with valid baselines were evaluated.'}

        # Calculate fast_1 score: fraction of correct and faster-than-baseline kernels
        p_threshold = 1.0
        fast_1_score = fastp(filtered_is_correct, filtered_baseline_speed, filtered_actual_speed, filtered_n, p_threshold)
        
        # Also calculate correctness rate (fast_0) as a secondary metric
        fast_0_score = np.sum(filtered_is_correct) / filtered_n
        
        # The combined_score for OpenEvolve will be the fast_1 score, as it's the most
        # representative of KernelBench's primary goal.
        combined_score = fast_1_score

        return {
            'combined_score': float(combined_score),
            'fast_1_score': float(fast_1_score),
            'correctness_rate': float(fast_0_score),
            'details': f'Evaluated {filtered_n} problems. fast_1 (correct & >1x speedup): {fast_1_score:.4f}, correctness rate: {fast_0_score:.4f}'
        }

    except Exception as e:
        # Catch any other errors during evaluation
        return {'combined_score': 0.0, 'error': f'Exception in evaluator: {str(e)}'}

# Note on Extending to Full Level 1:
# To evaluate on the full Level 1 set:
# 1. Set `PROBLEM_INDICES = range(len(KERNELBENCH_LEVEL_1_DATASET))`
# 2. Ensure `baseline_times` is populated with correct values from KernelBench's timing data.
# 3. The `fastp` calculation will then be over all 100 Level 1 problems.