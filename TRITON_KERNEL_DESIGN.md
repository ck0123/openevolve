# OpenEvolve + KernelBench: Generating Triton Kernels

This document outlines the plan and implementation details for using OpenEvolve to automatically generate and optimize Triton kernels for GPU, evaluated using the KernelBench benchmark (focusing on Level 1 problems). The evaluation strategy is designed to align with KernelBench's core metrics: correctness and performance, specifically using the `fast_p` metric.

## Overview

The goal is to leverage OpenEvolve's evolutionary algorithm, powered by Large Language Models (LLMs), to discover efficient Triton kernel implementations. These generated kernels will be tested for correctness and performance against standard PyTorch implementations using the KernelBench framework. The evaluation will adhere to KernelBench's established methodology to ensure fair and meaningful comparison.

### Core Components

1.  **OpenEvolve**: The evolutionary agent responsible for generating code variants.
2.  **KernelBench**: Provides the benchmark problems (Level 1), evaluation logic, reference implementations, and standardized metrics (like `fast_p`).
3.  **Integration Point**: A custom evaluator within OpenEvolve that bridges it with KernelBench.

## Implementation Plan

### 1. Project Structure

Work will be confined to the `/Users/qiaolina/Code/openevolve/examples/gen_triton_kernel` directory.

```
gen_triton_kernel/
├── initial_program.py  # Starting point for evolution (contains EVOLVE-BLOCK)
├── evaluator.py        # Evaluates generated kernels using KernelBench
├── config.yaml         # OpenEvolve configuration
└── KernelBench/        # The KernelBench repository
```

### 2. `initial_program.py`

This file provides the initial Triton kernel code for OpenEvolve to evolve. It must contain exactly one `EVOLVE-BLOCK`.

```python
# EVOLVE-BLOCK-START
import torch
import torch.nn as nn
import triton
import triton.language as tl

# Example Triton kernel: Element-wise addition
@triton.jit
def elementwise_add_kernel(
    a_ptr, b_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    a = tl.load(a_ptr + offsets, mask=mask)
    b = tl.load(b_ptr + offsets, mask=mask)
    output = a + b
    tl.store(output_ptr + offsets, output, mask=mask)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, a, b):
        output = torch.empty_like(a)
        assert a.is_cuda and b.is_cuda and output.is_cuda
        n_elements = output.numel()
        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
        elementwise_add_kernel[grid](
            a, b, output,
            n_elements,
            BLOCK_SIZE=1024,
        )
        return output
# EVOLVE-BLOCK-END

# Helper functions (not evolved)
def get_inputs():
    a = torch.randn(1, 128).cuda()
    b = torch.randn(1, 128).cuda()
    return [a, b]

def get_init_inputs():
    return []
```

### 3. `evaluator.py`

This is the crucial link between OpenEvolve and KernelBench. It takes the generated code, interfaces with KernelBench's evaluation functions, and returns a score for OpenEvolve, aligning with KernelBench's `fast_p` metric.

```python
import os
import sys
import importlib.util
import tempfile
import torch
import numpy as np
from src.eval import eval_kernel_against_ref, KernelExecResult # From KernelBench
from src.dataset import KERNELBENCH_LEVEL_1_DATASET, construct_kernelbench_dataset # From KernelBench
from src.score import fastp # Import KernelBench's scoring function

# For demonstration, we'll focus on a single problem or a small subset.
# In a full implementation, you might want to evaluate on a larger set or all of Level 1.
# PROBLEM_INDICES = range(len(KERNELBENCH_LEVEL_1_DATASET)) # Evaluate all Level 1 problems
PROBLEM_INDICES = [0] # Evaluate only the first problem for now

def evaluate(program_path: str) -> dict:
    """
    Evaluates a program generated by OpenEvolve using KernelBench's methodology.

    Args:
        program_path (str): Path to the Python file generated by OpenEvolve.

    Returns:
        dict: A dictionary containing evaluation metrics, primarily 'combined_score'.
              Adheres to KernelBench's focus on correctness and performance.
    """
    try:
        # 1. Read the generated code
        with open(program_path, 'r') as f:
            generated_code = f.read()

        # 2. Prepare to collect results for all selected problems
        eval_results_per_problem = []
        baseline_times = []
        
        # Assume a fixed baseline time for demonstration. 
        # In practice, these should be loaded from KernelBench's baseline files.
        # For a more accurate setup, you would load these from a JSON file like:
        # `results/timing/{hardware}/{baseline_time_torch}.json`
        # DUMMY_BASELINE_TIME_MS = 10.0
        
        # 3. Evaluate the generated kernel against each selected Level 1 problem
        for problem_index in PROBLEM_INDICES:
            reference_problem_path = KERNELBENCH_LEVEL_1_DATASET[problem_index]

            # 3a. Read the reference problem source code
            with open(reference_problem_path, 'r') as f:
                ref_arch_src = f.read()

            # 3b. Use KernelBench's evaluation function
            # Create a temporary file for the generated kernel code
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as tmp_file:
                tmp_file.write(generated_code)
                tmp_kernel_path = tmp_file.name

            # Read the generated code back as a string to pass to KernelBench
            with open(tmp_kernel_path, 'r') as f:
                custom_model_src = f.read()
            
            # Clean up the temporary file
            os.unlink(tmp_kernel_path)

            # 3c. Run KernelBench evaluation for this specific problem
            device = torch.device("cuda:0") # Assuming evaluation on GPU 0
            eval_result: KernelExecResult = eval_kernel_against_ref(
                original_model_src=ref_arch_src,
                custom_model_src=custom_model_src,
                seed_num=42,
                num_correct_trials=5, # Default from KernelBench for correctness
                num_perf_trials=10,   # Can be adjusted for speed during evolution
                verbose=False,
                measure_performance=True, # Essential for performance metric
                device=device
            )
            
            # Store the result for this problem
            eval_results_per_problem.append(eval_result)
            
            # For demonstration, use a dummy baseline. In practice, load from file.
            baseline_times.append(10.0) 

        # 4. Aggregate results across all problems to compute the final score
        # Following KernelBench's approach:
        # - `is_correct`: Boolean array indicating correctness for each problem.
        # - `baseline_speed`: Array of baseline execution times (from PyTorch) for each problem.
        # - `actual_speed`: Array of generated kernel execution times for each problem.
        # - `n`: Total number of problems evaluated.
        
        n = len(eval_results_per_problem)
        if n == 0:
             return {'combined_score': 0.0, 'error': 'No problems were evaluated.'}

        is_correct = np.array([res.correctness for res in eval_results_per_problem])
        # KernelBench uses `runtime` from KernelExecResult for `actual_speed`
        actual_speed = np.array([res.runtime if res.runtime and res.runtime > 0 else np.inf for res in eval_results_per_problem])
        # Baseline times need to be loaded from KernelBench's data
        baseline_speed = np.array(baseline_times) 
        
        # 5. Calculate combined_score using KernelBench's primary metric: fast_p
        # We will use fastp with p=1.0 as the primary score, which means 
        # "fraction of tasks that are both correct and faster than PyTorch baseline".
        # This directly aligns with KernelBench's core benchmark metric.
        
        # Handle case where baseline speed is not available or zero
        valid_baseline = (baseline_speed > 0) & np.isfinite(baseline_speed)
        if not np.any(valid_baseline):
            # If no valid baselines, fall back to correctness rate (fast_0)
            fast_0_score = np.sum(is_correct) / n
            return {
                'combined_score': float(fast_0_score),
                'details': f'Correctness rate (fast_0) due to missing baselines: {fast_0_score:.4f}',
                'correctness_rate': float(fast_0_score)
            }
            
        # Filter arrays to only include problems with valid baselines
        filtered_is_correct = is_correct[valid_baseline]
        filtered_baseline_speed = baseline_speed[valid_baseline]
        filtered_actual_speed = actual_speed[valid_baseline]
        filtered_n = len(filtered_is_correct)
        
        if filtered_n == 0:
             return {'combined_score': 0.0, 'error': 'No problems with valid baselines were evaluated.'}

        # Calculate fast_1 score: fraction of correct and faster-than-baseline kernels
        p_threshold = 1.0
        fast_1_score = fastp(filtered_is_correct, filtered_baseline_speed, filtered_actual_speed, filtered_n, p_threshold)
        
        # Also calculate correctness rate (fast_0) as a secondary metric
        fast_0_score = np.sum(filtered_is_correct) / filtered_n
        
        # The combined_score for OpenEvolve will be the fast_1 score, as it's the most
        # representative of KernelBench's primary goal.
        combined_score = fast_1_score

        return {
            'combined_score': float(combined_score),
            'fast_1_score': float(fast_1_score),
            'correctness_rate': float(fast_0_score),
            'details': f'Evaluated {filtered_n} problems. fast_1 (correct & >1x speedup): {fast_1_score:.4f}, correctness rate: {fast_0_score:.4f}'
        }

    except Exception as e:
        # Catch any other errors during evaluation
        return {'combined_score': 0.0, 'error': f'Exception in evaluator: {str(e)}'}

# Note on Extending to Full Level 1:
# To evaluate on the full Level 1 set:
# 1. Set `PROBLEM_INDICES = range(len(KERNELBENCH_LEVEL_1_DATASET))`
# 2. Ensure `baseline_times` is populated with correct values from KernelBench's timing data.
# 3. The `fastp` calculation will then be over all 100 Level 1 problems.
```

### 4. `config.yaml`

Configuration for the OpenEvolve run, including LLM settings and evolution parameters.

```yaml
# Evolution settings
max_iterations: 50 # Start small for testing
checkpoint_interval: 10
parallel_evaluations: 1 # Adjust based on GPU resources

# LLM configuration - Using local vLLM
llm:
  api_base: "http://localhost:8000/v1" # Replace with your vLLM address/port
  api_key: "none" # vLLM typically doesn't require an API Key
  models:
    - name: "your_triton_expert_model_name" # Replace with your model name
      weight: 1.0
  temperature: 0.7
  max_tokens: 4096
  timeout: 120
  retries: 3
  retry_delay: 5

# Database configuration (MAP-Elites algorithm)
database:
  population_size: 30
  num_islands: 2
  migration_interval: 5
  feature_dimensions:
    - "score"
    - "complexity" # Or other dimensions

# Evaluation settings
evaluator:
  timeout: 300 # KernelBench evaluation can be time-consuming
  max_retries: 2

# Prompt configuration
prompt:
  system_message: |
    You are an expert Triton kernel developer. Your task is to write efficient Triton kernels to replace PyTorch operators in given neural network architectures. 
    The goal is to optimize the architecture named 'Model' by replacing its operations with custom Triton kernels to achieve speedups.
    You have complete freedom to choose which operators to replace. Consider operator fusion opportunities or algorithmic changes. Your optimized architecture must be named 'ModelNew'.
    You will be provided with a reference PyTorch implementation (class Model) and example inputs.
    Your response should be a complete, syntactically correct Python file containing the 'ModelNew' class with Triton kernels.
    Only output the new model code, no other text, and NO testing code!
    Focus on correctness first, then performance. The kernel must produce numerically identical results to the reference implementation.
    Here is an example of the expected format:
    ```
    import torch
    import torch.nn as nn
    import triton
    import triton.language as tl

    @triton.jit
    def your_kernel(...):
        ...

    class ModelNew(nn.Module):
        def __init__(self):
            super().__init__()
            ...

        def forward(self, ...):
            ...
            your_kernel[grid](...)
            ...
            return output
    ```
    Ensure the code compiles and runs correctly with the provided input shapes.
  num_top_programs: 3
  num_diverse_programs: 2

# Logging
log_level: "INFO"
```

## Running the System

1.  **Start vLLM Server**: Ensure your vLLM service is running with the desired model loaded.
2.  **Prepare KernelBench Environment**: Make sure the KernelBench dependencies are installed.
3.  **Run OpenEvolve**:
    ```bash
    cd /Users/qiaolina/Code/openevolve
    python openevolve-run.py examples/gen_triton_kernel/initial_program.py examples/gen_triton_kernel/evaluator.py --config examples/gen_triton_kernel/config.yaml --iterations 50
    ```
4.  **Monitor and Analyze**: Observe logs and checkpoints to track the evolution of kernel performance.

## Key Considerations & Optimizations

*   **LLM Prompt Engineering**: The `system_message` in `config.yaml` is critical. It must clearly guide the LLM to produce correct and efficient Triton code.
*   **Evaluation Efficiency**: KernelBench evaluations are slow. Use `parallel_evaluations` and reduce `num_perf_trials` during initial development.
*   **Robust Error Handling**: The `evaluator.py` must gracefully handle various types of errors from generated code (compile, runtime) and provide clear feedback.
*   **Baseline Timings**: Accurate baseline timings are essential for the `fast_p` metric. Load these from KernelBench's precomputed baseline files (e.g., `results/timing/{hardware}/{baseline}.json`).
*   **Feature Dimensions**: Define meaningful `feature_dimensions` in `config.yaml` to guide the MAP-Elites algorithm effectively.
*   **Scoring Metric Alignment**: The `evaluator.py` is designed to use KernelBench's primary metric, `fast_p` (specifically `fast_1`), ensuring alignment with the benchmark's goals. This provides a clear, standardized measure of success for the evolutionary process.
*   **Multi-Problem Evaluation**: The updated `evaluator.py` structure supports evaluating against multiple or all Level 1 problems, which is the standard practice in KernelBench for a comprehensive assessment.